---
title: "Wilcoxon Wars"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, 
                      include = TRUE, fig.align = "center")

library(prmisc)
library(moments)

source("wilcoxon_vis.R")
```

## Prologue
To explain how p-values work on my [German science blog](www.einglasrotwein.de), I compared the simulated ratings of two burger restaurants with each other. (**Update:** I did an English version of the original post. Find it [here](https://einglasrotwein.de/what-the-p-value-cannot-tell-you/). Both restaurants were rated on a scale from 1 - 10. Then, someone on Twitter asked if it wouldn't have been appropriate to use a nonparametric test instead of the t-test I applied, as the ratings were ordinal data. However, as I had drawn fictional data from a normal distribution, I knew it was interval data. Furthermore, I remembered the paper by [Fagerland (2012)](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-12-78) which showed the Wilcoxon test to suck for skewed data with increasing sample size - while the t-test was unaffected.

Okay, to be precise, the Wilcoxon test did not "suck", but rather answered the different question of Prob(X < Y) = .5 instead of if the groups differ in medians. However, I don't think many people know that (I didn't before I read Fagerland's paper) and are - like me - just told in their statistic lectures to calculate a Wilcoxon test when the assumptions for a t-test are not met. This can lead to ... quite unintended results, as we'll see.

For by blog however, I just wanted to prove that the t-test was the appropriate choice for my simulation (#1). At first. However, I was surprised that it didn't fail, even if I converted the data to be plain ugly ordinal (#2). Then, things got out of hand and I turned to the holy war to bring. That. Wilcoxon. Test. Down.

Make sure to read Fagerland's paper for a far more rigorous and systematic approach with theoretical background. Find the code for my simulations in this [GitHub repo](https://github.com/einGlasRotwein/bits-and-pieces). But for now:

This is Sparta.

### The simulation
A population of 10 million ratings is generated for each restaurant. Then, samples of the following sizes (per restaurant) are drawn: 10, 50, 100, 200, 500, 1000, 2000, 3000, 4000. For each sample size, 10,000 samples of the respective size are drawn. Then, the two restaurants are compared with both, a t-test (Welch test) and a Wilcoxon rank sum (Mann Whitney U) test for independent samples. Later, I added a permutation test (`independence_test` (`distribution = "approximate"`) from the package `coin`) and a bootstrapped t-test (`boot.ttest2` from the package `Rfast`). See "Under Construction".

In seven different scenarios, parameters of the underlying populations are varied to examine the robustness of the two tests.

## Under Construction
When [Dale Barr](https://twitter.com/dalejbarr/status/1137672493857234944) came up with the very valid remark that the superior performance of the t-test might be due to `var.equal = FALSE`, I added another t-test with `var.equal = TRUE`. And then, quite a lot of people (e.g. [Anders Eklund](https://twitter.com/wandedob/status/1137726363157377025), [David Colquhoun](https://twitter.com/david_colquhoun/status/1137757426982236162) and [Devin Didericksen](https://twitter.com/didericksen/status/1137765263426953218)) pointed out that a permutation test would have been a good call. However, there was also the [objection](https://twitter.com/skornblith/status/1137768203956998147) that due to the different distributions, observations are not interchangeable and bootstrapping might work better. Hence, I added another round with a `independence_test` (`distribution = "approximate"`) from the package `coin`. And one more with `boot.ttest2` from the package `Rfast`. [Lars Juhl Jensen](https://twitter.com/larsjuhljensen/status/1138031870044717056) pointed out that in some of my scenarios, the means of my populations are the same, but the medians differ. So, I'm fixing that for the next runs.

This requires the analyses to be re-run, which takes some time. I'll update stuff step by step.

### TO DO
- Permutation test and bootstrap from scenario 5 onwards.
- Same median for populations 5 - 7
    - Turns out, this is harder than it looks. I found a function meant to create something like this (and cheated a bit with rounding), but the distribution looks rather normal than lognormal. I included the example as 04a and keep working on the last scenarios.

## 01 - default
```{r}
set.seed(kyd)
pop1 <- rnorm(pop, 5.5, 2.4)
pop1 <- burger(pop1)
```

Normal distribution; integers from 1 - 10. Population 1 and 2 are identical. Both populations: `r print_mean_sd(pop1, parentheses = FALSE)`, median = `r round(median(pop1), 2)`, skewness = `r round(skewness(pop1), 2)`.

```{r, out.width = '50%'}
plot_pop01
```

```{r}
plot_out01
```

## 02 - ordinalskal scale
Same populations as in #01, but converted to an ugly ordinal scale according to the following catagories:

| old rating | label     | new rating |
| :--------- | :-------- | :--------- |
| 1 -  4     | poor      | 1          |
| 5 -  8     | good      | 2          |
| 9 - 10     | excellent | 3          |

```{r}
set.seed(kyd)
pop1 <- ordinal_burgers(pop1)
```

Both populations: `r print_mean_sd(pop1, parentheses = FALSE)`, median = `r round(median(pop1), 2)`, skewness = `r round(skewness(pop1), 2)`.

```{r out.width = '50%'}
plot_pop02
```

```{r}
plot_out02
```

## 03 - extremely skewed lognormal
```{r}
set.seed(kyd)
pop1 <- rlnorm(pop)
```

Extremely skewed lognormal distribution (skewness = `r round(skewness(pop1), 2)`). Not converted to integers this time. Both populations: `r print_mean_sd(pop1, parentheses = FALSE)`, median = `r round(median(pop1), 2)`.

```{r}
plot_pop03 +
  facet_zoom(xlim = c(0, 15))
```

```{r}
plot_out03
```

## 04 - different SDs normal distribution
```{r}
set.seed(kyd)
pre04_1 <- rnorm(pop, 5.5, .5)
post04_1 <- burger(pre04_1)

pre04_2 <- rnorm(pop, 5.5, 3)
post04_2 <- burger(pre04_2)
```

Different standard deviations between populations. Right after sampling: population 1 `r print_mean_sd(pre04_1, parentheses = FALSE)`, median = `r round(median(pre04_1), 2)`, skewness = `r round(skewness(pre04_1))`; population 2 `r print_mean_sd(pre04_2, parentheses = FALSE)`, median = `r round(median(pre04_2), 2)`, skewness = `r round(skewness(pre04_2))`. After conversion to integers from 1 - 10: population 1 `r print_mean_sd(post04_1, parentheses = FALSE)`, median = `r round(median(post04_1), 2)`, skewness = `r round(skewness(post04_1))`; population 2 `r print_mean_sd(post04_2, parentheses = FALSE)`, median = `r round(median(post04_2), 2)`, skewness = `r round(skewness(post04_2))`.

```{r}
plot_pop04
```

```{r}
plot_out04
```

## 04a - different SDs, still kinda normal
```{r}
params1 <- lognorm_spec(40, .5)
params2 <- lognorm_spec(40, 1)

set.seed(kyd)
pop1 <- round(rlnorm(pop, params1[1], params1[2]))
pop2 <- round(rlnorm(pop, params2[1], params2[2]))
```

Initially, I tried to create lognormal distributions with a specified mean and sd (and the same median). To achieve the exakt same means and medians, I rounded the distribution. Turns out, they were rather normal than anything else. I included the example nevertheless: population 1 `r print_mean_sd(pop1, parentheses = FALSE)`, median = `r round(median(pop1), 2)`, skewness = `r round(skewness(pop1), 2)`; population 2 `r print_mean_sd(pop2, parentheses = FALSE)`, median = `r round(median(pop2), 2)`, skewness = `r round(skewness(pop2), 2)`.

```{r}
plot_pop04a
```

```{r}
plot_out04a
```

## 05 - Different lognormal distributions
```{r}
params1 <- lognorm_spec(40, .5)
params2 <- lognorm_spec(40, 1)

set.seed(kyd)
pop1 <- round(rlnorm(pop, params1[1], params1[2]))
pop2 <- round(rlnorm(pop, params2[1], params2[2]))
```

Similar to Fagerland: Two lognormal distributions with different SDs, but same mean and median: population 1 `r print_mean_sd(pop1, parentheses = FALSE)`, median = `r round(median(pop1), 2)`, skewness = `r round(skewness(pop1), 2)`; population 2 `r print_mean_sd(pop2, parentheses = FALSE)`, median = `r round(median(pop2), 2)`, skewness = `r round(skewness(pop2), 2)`.

```{r}
plot_pop05_01
```

```{r}
plot_pop05_02
```

```{r}
plot_out05
```

## 06 - Two different gamma distributions
```{r}
set.seed(kyd)
pop1 <- rgamma(pop, shape = .24)
pop2 <- rgamma(pop, shape = .42)

diff <- mean(pop1) - mean(pop2)
pop2 <- pop2 + diff
```

Similar to Fagerland: Two lognormal distributions with different SDs. Population 2 was shifted to the same mean as population 1. Note that Fagerland had a more elegant approach where the two distributions differed only in SD while mine also have a different skewness. population 1: `r print_mean_sd(pop1)`, skewness = `r round(skewness(pop1), 2)`; population 2: `r print_mean_sd(pop2)`, skewness = `r round(skewness(pop2), 2)`.

```{r}
plot_pop06_01
```

```{r}
plot_pop06_02
```

```{r}
plot_out06
```

## 07 - Fagerland inspired escalation
```{r}
set.seed(kyd)
pop1 <- rlnorm(pop, sdlog = .72)
pop2 <- rgamma(pop, shape = .42)

diff <- mean(pop1) - mean(pop2)
pop2 <- pop2 + diff
```

Both distributions used Fagerland's paper: A lognormal distribution (population 1) and a gamma distribution (population 2). Population 2 was shifted to the same mean as population 1. Skewness was comparable for both. population 1: `r print_mean_sd(pop1)`, skew = `r round(skewness(pop1), 2)`; population 2: `r print_mean_sd(pop2)`, skew = `r round(skewness(pop2), 2)`.

```{r}
plot_pop07_01
```

```{r}
plot_pop07_02
```


```{r}
plot_out07
```
